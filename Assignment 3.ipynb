{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e7877d",
   "metadata": {},
   "source": [
    "# Assignmnet 3 (100 + 5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b6d20",
   "metadata": {},
   "source": [
    "**Name:** <br>\n",
    "**Email:** <br>\n",
    "**Group:** A/B <br>\n",
    "**Hours spend *(optional)* :** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f79f88",
   "metadata": {},
   "source": [
    "### Question 1: Transformer model *(100 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd0c1a",
   "metadata": {},
   "source": [
    "As a Machine Learning engineer at a tech company, you were given a task to develop a machine translation system that translates **English (source) to German (Target)**. You can use existing libraries but the training needs to be done from scratch (usage of pretrained weights is not allowed). You have the freedom to select any dataset for training the model. Use a small subset of data as a validation dataset and report the BLEU score on the validation set. Also, provide a short description of your transformer model architecture, hyperparameters, and training (also provide the training loss curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a6e95c",
   "metadata": {},
   "source": [
    "<h3> Submission </h3>\n",
    "\n",
    "The test set **(test.txt)** will be released one week before the deadline. You should submit the output of your model on the test set separately. Name the output file as **\"first name_last_name_test_result.txt\"**. Each line of the submission file should contain only the translated text of the corresponding sentence from 'test.txt'.\n",
    "\n",
    "The 'first name_last_name_test_result.txt' file will be evaluated by your instructor and the student who could get the best BLEU score will get 5 additional points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4d2da",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "Here are some of the parallel datasets (see Datasets and Resources file):\n",
    "* Europarl Parallel corpus - https://www.statmt.org/europarl/v7/de-en.tgz\n",
    "* News Commentary - https://www.statmt.org/wmt14/training-parallel-nc-v9.tgz (use DE-EN parallel data)\n",
    "* Common Crawl corpus - https://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz (use DE-EN parallel data)\n",
    "\n",
    "You can also use other datasets of your choice. In the above datasets, **'.en'** file has the text in English, and **'.de'** file contains their corresponding German translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f05905b",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "1) You can also consider using a small subset of the dataset if the training dataset is large\n",
    "2) Sometimes you can also get out of memory errors while training, so choose the hyperparameters carefully.\n",
    "3) Your training will be much faster if you use a GPU. If you are using a CPU, it may take several hours or even days. (you can also use Google Colab GPUs for training. link: https://colab.research.google.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d1cbe-fe18-402e-8952-6367e3bf783f",
   "metadata": {},
   "source": [
    "**MY MACHINE TRANSLATION SYSTEM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78c6fc1-b488-4dfd-90d3-50cb42aad2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ecf2fc-148b-4868-9140-78a648d2d599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n",
      "201854\n",
      "201995\n"
     ]
    }
   ],
   "source": [
    "#loading training data\n",
    "with open('news-commentary-v9.de-en.de', 'r', encoding='utf-8') as file1:\n",
    "    german_corpus = file1.readlines()\n",
    "\n",
    "\n",
    "with open('news-commentary-v9.de-en.en', 'r', encoding='utf-8') as file1:\n",
    "    english_corpus = file1.readlines()\n",
    "\n",
    "print('Data Loaded')\n",
    "print(len(german_corpus))\n",
    "print(len(english_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d83d6b34-0a16-40e5-b17e-99c874e43d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  $10,000 Gold?\n",
      "\n",
      "small_vocab_de Line 1:  Steigt Gold auf 10.000 Dollar?\n",
      "\n",
      "small_vocab_en Line 2:  SAN FRANCISCO – It has never been easy to have a rational conversation about the value of gold.\n",
      "\n",
      "small_vocab_de Line 2:  SAN FRANCISCO – Es war noch nie leicht, ein rationales Gespräch über den Wert von Gold zu führen.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sample sentences\n",
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1,\n",
    "                                               english_corpus[sample_i]))\n",
    "    print('small_vocab_de Line {}:  {}'.format(sample_i + 1, \n",
    "                                               german_corpus[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de47bb16-9631-42d2-a05d-686ee3f4d665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English words:  4386024\n",
      "Unique English words:  152217\n",
      "Total German words :  4500845\n",
      "Unique German words:  245674\n"
     ]
    }
   ],
   "source": [
    "\n",
    "english_words = [word for sentence in english_corpus for word in sentence.split()]\n",
    "english_words_counter = collections.Counter(english_words)\n",
    "\n",
    "german_words = [word for sentence in german_corpus for word in sentence.split()]\n",
    "german_words_counter = collections.Counter(german_words)\n",
    "\n",
    "print(\"Total English words: \", len(english_words))\n",
    "print(\"Unique English words: \", len(english_words_counter))\n",
    "print(\"Total German words : \", len(german_words))\n",
    "print(\"Unique German words: \", len(german_words_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5301dc1a-64d9-4d2a-8ee8-df4f0351ae45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201854\n",
      "201854\n",
      "Data split into val and train set \n"
     ]
    }
   ],
   "source": [
    "min_length = min(len(german_corpus), len(english_corpus))\n",
    "\n",
    "german_corpus = german_corpus[:min_length]\n",
    "english_corpus = english_corpus[:min_length]\n",
    "\n",
    "src = german_corpus\n",
    "tgt = english_corpus\n",
    "print(len(german_corpus))\n",
    "print(len(english_corpus))\n",
    "\n",
    "# Splitting into training and validation sets with a ratio of 80-20\n",
    "src_train, src_val, tgt_train, tgt_val = train_test_split(src, \n",
    "                                                          tgt, \n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=42)\n",
    "print(\"Data split into val and train set \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edd856ba-9b36-4a4e-ba67-2531b5d96379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English words after preprocessing:  4313195\n",
      "Unique English words:  63324\n",
      "Total German words after preprocessing:  4418492\n",
      "Unique German words:  144204\n"
     ]
    }
   ],
   "source": [
    "#preprocessing\n",
    "def preprocess_and_tokenize(corpus):\n",
    "    tokenized_corpus = []\n",
    "    for sentence in corpus:\n",
    "        # Converting to lowercase\n",
    "        sentence = sentence.lower()\n",
    "        # Removing non-printable characters\n",
    "        sentence = ''.join(filter(lambda x: x in string.printable, sentence))\n",
    "        # Removing punctuation\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "        # Tokenizing\n",
    "        words = word_tokenize(sentence)\n",
    "        # Removing non-alphabetic tokens\n",
    "        words = [word for word in words if word.isalpha()]\n",
    "        # Handle empty sentences after preprocessing\n",
    "        if len(words) == 0:\n",
    "            words = []\n",
    "        tokenized_corpus.append(words)\n",
    "    return tokenized_corpus\n",
    "\n",
    "src_train = preprocess_and_tokenize(src_train)\n",
    "tgt_train = preprocess_and_tokenize(tgt_train)\n",
    "src_val = preprocess_and_tokenize(src_val)\n",
    "tgt_val = preprocess_and_tokenize(tgt_val)\n",
    "\n",
    "\n",
    "english_processed = preprocess_and_tokenize(english_corpus)\n",
    "german_processed = preprocess_and_tokenize(german_corpus)\n",
    "\n",
    "#vocabulary\n",
    "english_vocab = [word for sentence in english_processed for word in sentence]\n",
    "german_vocab = [word for sentence in german_processed for word in sentence]\n",
    "english_words_counter = collections.Counter(english_vocab)\n",
    "german_words_counter = collections.Counter(german_vocab)\n",
    "\n",
    "print(\"Total English words after preprocessing: \", len(english_vocab))\n",
    "print(\"Unique English words: \", len(english_words_counter))\n",
    "print(\"Total German words after preprocessing: \", len(german_vocab))\n",
    "print(\"Unique German words: \", len(german_words_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bbda846-4c0a-44e3-82b8-a7275d1a90a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleLSTM(\n",
      "  (embedding): Embedding(63325, 256)\n",
      "  (rnn): LSTM(256, 512)\n",
      "  (fc_out): Linear(in_features=512, out_features=144205, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# LSTM model\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        prediction = self.fc_out(output)\n",
    "        return prediction\n",
    "\n",
    "INPUT_DIM = len(english_words_counter) + 1\n",
    "OUTPUT_DIM = len(german_words_counter) + 1\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "\n",
    "model = SimpleLSTM(INPUT_DIM, EMB_DIM, HID_DIM, OUTPUT_DIM)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42588d6e-a304-4d57-a0d4-60e19a4ed791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sting to interger\n",
    "english_stoi = {word: index+1 for index, (word, _)\n",
    "                in enumerate(english_words_counter.most_common())}\n",
    "german_stoi = {word: index+1 for index, (word, _) \n",
    "               in enumerate(german_words_counter.most_common())}\n",
    "\n",
    "train_english_sentences = [[english_stoi[word] for word in sentence] \n",
    "                           for sentence in tgt_train]\n",
    "train_german_sentences = [[german_stoi[word] for word in sentence] \n",
    "                          for sentence in src_train]\n",
    "val_english_sentences = [[english_stoi[word] for word in sentence] \n",
    "                         for sentence in tgt_val]\n",
    "val_german_sentences = [[german_stoi[word] for word in sentence]\n",
    "                        for sentence in src_val]\n",
    "\n",
    "\n",
    "train_english_sentences_pad = pad_sequence([torch.as_tensor(s) \n",
    "                                            for s in train_english_sentences], \n",
    "                                           padding_value=0)\n",
    "train_german_sentences_pad = pad_sequence([torch.as_tensor(s) \n",
    "                                           for s in train_german_sentences], \n",
    "                                          padding_value=0)\n",
    "\n",
    "val_english_sentences_pad = pad_sequence([torch.as_tensor(s) \n",
    "                                          for s in val_english_sentences], \n",
    "                                         padding_value=0)\n",
    "val_german_sentences_pad = pad_sequence([torch.as_tensor(s) \n",
    "                                         for s in val_german_sentences], \n",
    "                                        padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "961f42a4-9708-4fe5-860e-4c52d4d6bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data = TensorDataset(torch.transpose(train_english_sentences_pad, 0, 1),\n",
    "                           torch.transpose(train_german_sentences_pad, 0, 1))\n",
    "val_data = TensorDataset(torch.transpose(val_english_sentences_pad, 0, 1), \n",
    "                         torch.transpose(val_german_sentences_pad, 0, 1))\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a42dd681-d5b9-436a-9e94-a27d2c7063f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (4896) to match target batch_size (4288).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 62\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_loader, criterion, device)\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     13\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     14\u001b[0m tgt \u001b[38;5;241m=\u001b[39m tgt\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aai2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aai2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aai2\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aai2\\lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (4896) to match target batch_size (4288)."
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        tgt = tgt.view(-1)\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src)\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            tgt = tgt.view(-1)\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(val_loader)\n",
    "\n",
    "# Calculate BLEU score\n",
    "def calculate_bleu_score(model, val_loader, english_itos, german_itos, device):\n",
    "    model.eval()\n",
    "    references, hypotheses = [], []\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src)\n",
    "            output = output.argmax(dim=-1)\n",
    "            for i in range(tgt.size(0)):\n",
    "                ref = [german_itos[idx] for idx in tgt[i].cpu().numpy() if idx != 0]\n",
    "                hyp = [german_itos[idx] for idx in output[i].cpu().numpy() if idx != 0]\n",
    "                references.append([ref])\n",
    "                hypotheses.append(hyp)\n",
    "    return corpus_bleu(references, hypotheses)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "# Calculate BLEU score\n",
    "english_itos = {idx: word for word, idx in english_stoi.items()}\n",
    "german_itos = {idx: word for word, idx in german_stoi.items()}\n",
    "bleu_score = calculate_bleu_score(model, val_loader, english_itos, german_itos, device)\n",
    "print(f'BLEU score = {bleu_score * 100:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
